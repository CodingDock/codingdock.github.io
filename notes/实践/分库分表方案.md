这是转载自[美团技术团队Blog]([https://tech.meituan.com](https://tech.meituan.com/))的文章。怕他删除，做个备份。同时我对文章做了一些补充说明。



## 大众点评订单系统分库分表实践

2016年11月18日 作者: 华慰 [文章链接](https://tech.meituan.com/2016/11/18/dianping-order-db-sharding.html) 3307字 7分钟阅读

原大众点评的订单单表早就已经突破两百G，由于查询维度较多，即使加了两个从库，优化索引，仍然存在很多查询不理想的情况。去年大量抢购活动的开展，使数据库达到瓶颈，应用只能通过限速、异步队列等对其进行保护；业务需求层出不穷，原有的订单模型很难满足业务需求，但是基于原订单表的DDL又非常吃力，无法达到业务要求。随着这些问题越来越突出，订单数据库的切分就愈发急迫了。

这次切分，我们的目标是未来十年内不需要担心订单容量的问题。

先对订单库进行垂直切分，将原有的订单库分为基础订单库、订单流程库等，本文就不展开讲了。
![垂直切分](1b24c0df.png)

垂直切分



垂直切分缓解了原来单集群的压力，但是在抢购时依然捉襟见肘。原有的订单模型已经无法满足业务需求，于是我们设计了一套新的统一订单模型，为同时满足C端用户、B端商户、客服、运营等的需求，我们分别通过用户ID和商户ID进行切分，并通过PUMA（我们内部开发的MySQL binlog实时解析服务）同步到一个运营库。
![水平切分](8aff3f22.png)

水平切分



## 切分策略

### 1. 查询切分

将ID和库的Mapping关系记录在一个单独的库中。
![查询切分](ddb1ceb8.png)

查询切分



优点：ID和库的Mapping算法可以随意更改。
缺点：引入额外的单点。

### 2. 范围切分

比如按照时间区间或ID区间来切分。
![范围切分](f80cafa2.png)

范围切分



优点：单表大小可控，天然水平扩展。
缺点：无法解决集中写入瓶颈的问题。

### 3. Hash切分

一般采用Mod来切分，下面着重讲一下Mod的策略。
![hash切分](52610d6d.png)

hash切分



数据水平切分后我们希望是一劳永逸或者是易于水平扩展的，所以推荐采用mod 2^n这种一致性Hash。

以统一订单库为例，我们分库分表的方案是32*32的，即通过UserId后四位mod 32分到32个库中，同时再将UserId后四位Div 32 Mod 32将每个库分为32个表，共计分为1024张表。线上部署情况为8个集群(主从)，每个集群4个库。

为什么说这种方式是易于水平扩展的呢？我们分析如下两个场景。

#### 场景一：数据库性能达到瓶颈

#### 方法一

按照现有规则不变，可以直接扩展到32个数据库集群。
![扩展方法](b301cb95.png)

扩展方法



#### 方法二

如果32个集群也无法满足需求，那么将分库分表规则调整为(32*2^n)*(32⁄2^n)，可以达到最多1024个集群。
![扩展方法](c573941e.png)

扩展方法



#### 场景二：单表容量达到瓶颈（或者1024已经无法满足你）

#### 方法：

![扩展方法](494ebd9e.png)

扩展方法



假如单表都已突破200G，200*1024=200T（按照现有的订单模型算了算，大概一万千亿订单，相信这一天，嗯，指日可待！），没关系，32*(32*2^n)，这时分库规则不变，单库里的表再进行裂变，当然，在目前订单这种规则下（用userId后四位 mod）还是有极限的，因为只有四位，所以最多拆8192个表，至于为什么只取后四位，后面会有篇幅讲到。

另外一个维度是通过ShopID进行切分，规则8*8和UserID比较类似，就不再赘述，需要注意的是Shop库我们仅存储了订单主表，用来满足Shop维度的查询。

## 唯一ID方案

这个方案也很多，主流的有那么几种:

### 1. 利用数据库自增ID

优点：最简单。 缺点：单点风险、单机性能瓶颈。

### 2. 利用数据库集群并设置相应的步长（Flickr方案）

优点：高可用、ID较简洁。 缺点：需要单独的数据库集群。

### 3. Twitter Snowflake

优点：高性能高可用、易拓展。 缺点：需要独立的集群以及ZK。

### 4. 一大波GUID、Random算法

优点：简单。 缺点：生成ID较长，有重复几率。

### 我们的方案

为了减少运营成本并减少额外的风险我们排除了所有需要独立集群的方案，采用了带有业务属性的方案： > 时间戳+用户标识码+随机数

有下面几个好处：

- 方便、成本低。
- 基本无重复的可能。
- 自带分库规则，这里的用户标识码即为用户ID的后四位，在查询的场景下，只需要订单号就可以匹配到相应的库表而无需用户ID，只取四位是希望订单号尽可能的短一些，并且评估下来四位已经足够。
- 可排序，因为时间戳在最前面。

当然也有一些缺点，比如长度稍长，性能要比int/bigint的稍差等。

## 其他问题

- 事务支持：我们是将整个订单领域聚合体切分，维度一致，所以对聚合体的事务是支持的。
- 复杂查询：垂直切分后，就跟join说拜拜了；水平切分后，查询的条件一定要在切分的维度内，比如查询具体某个用户下的各位订单等；禁止不带切分的维度的查询，即使中间件可以支持这种查询，可以在内存中组装，但是这种需求往往不应该在在线库查询，或者可以通过其他方法转换到切分的维度来实现。

## 数据迁移

数据库拆分一般是业务发展到一定规模后的优化和重构，为了支持业务快速上线，很难一开始就分库分表，垂直拆分还好办，改改数据源就搞定了，一旦开始水平拆分，数据清洗就是个大问题，为此，我们经历了以下几个阶段。

### 第一阶段

![数据迁移](2ae1ba98.png)

数据迁移



- 数据库双写（事务成功以老模型为准），查询走老模型。
- 每日job数据对账（通过DW），并将差异补平。
- 通过job导历史数据。

### 第二阶段

![数据迁移](0a93304e.png)

数据迁移



- 历史数据导入完毕并且数据对账无误。
- 依然是数据库双写，但是事务成功与否以新模型为准，在线查询切新模型。
- 每日job数据对账，将差异补平。

### 第三阶段

![数据迁移](a8d54b30.png)

数据迁移



- 老模型不再同步写入，仅当订单有终态时才会异步补上。
- 此阶段只有离线数据依然依赖老的模型，并且下游的依赖非常多，待DW改造完就可以完全废除老模型了。

## 总结

并非所有表都需要水平拆分，要看增长的类型和速度，水平拆分是大招，拆分后会增加开发的复杂度，不到万不得已不使用。

在大规模并发的业务上，尽量做到在线查询和离线查询隔离，交易查询和运营/客服查询隔离。

拆分维度的选择很重要，要尽可能在解决拆分前问题的基础上，便于开发。

数据库没你想象的那么坚强，需要保护，尽量使用简单的、良好索引的查询，这样数据库整体可控，也易于长期容量规划以及水平扩展。

**最后感谢一下棒棒的DBA团队和数据库中间件团队对项目的大力协助！**



来源：

[大众点评订单系统分库分表实践](<https://tech.meituan.com/2016/11/18/dianping-order-db-sharding.html>)





## 补充说明：

分库分表的方案是32*32的，即通过UserId后四位mod 32分到32个库中，同时再将UserId后四位Div 32 Mod 32将每个库分为32个表，共计分为1024张表。后续都以2^n为倍数进行调整，**为什么文中说这种搞法有优势**？**因为在扩容时容易进行数据迁移，而且不需要大幅调整应用的映射逻辑。**

32库*32表。共1024张表。假设单表可支持1000W数据，这样总共能支持1万亿数据^_^。可见这足以满足一般需求。

结合文中举例说明：

#### 第一阶段：8集群>>>32集群

部署8个数据库集群（**主从，下同**），每个集群分配4个库，即每个集群就容纳了4\*32张表。数据体量上来后，达到集群性能瓶颈，需要对集群扩容。因为每个集群部署了多个库，此时增加集群数量同时减少单个集群的数据库数量就能提升性能。在不改变库表比例32\*32的情况下，最多能扩容到32个集群，每个集群一个库。**故迁移方案为：增加集群数目到32，直接迁移各个库到新集群，不需要调整表。这样，应用只需要修改数据源映射算法即可完成。**

#### 第二阶段：32\*32 >>> 1024\*1

数据体量继续增加，而现在已经是一个集群一个库。再扩容可以采用增加集群数量的同时减少库内表数量的方法。以2^n为倍数进行调整，方案为：库数目\*表数目=(32\*2^n)(32/2^n)；比如：64库\*16表、128库\*8表.....**1024库\*1表**（极限方案）。
我们以1024库\*1表这个极限方案说明。此时共部署1024个集群，每个集群一个库，每库一张表。数据分布逻辑不变：`key % 库数量 = 库号，(key / 库数量 ) % 表数量 = 表号 `
原来的分库分表方案：32\*32
`key % 32 = 库，(key / 32 ) % 32 = 表 `
调整为，分库分表方案：1024\*1
`key % 1024 = 库，(key / 1024) % 1= 表 `
可以根据公式计算得出结果：老方案下同一张表的数据在新方案下依然能映射到同一张表，但是会位于不同的库号和表号（这不是废话么）。所以迁移只需要移动整张表即可。例子：取老库表内的一条记录的key（可以不取，能直接计算出来，如：原3号库8号表的key=32\*32 **0** +32\*8+3=259；公式内的 **0** 表示第一条记录），计算出新库号=259%1024=259；表号=(259/1024)%1=0；即原3号库8号表迁移到259号库0号表。**迁移方案：迁移老库的表到计算出的新库即可，应用需要修改数据的库表映射算法，需要修改数据源映射算法。**

#### 第三阶段：1024\*1>>>>>1024\*2^n

以n=0，即迁移为1024\*2，表数量扩大一倍为2048张，共1024个库（集群），每库两张。此时，可以根据上文的算法得出结论：原来的一张表直接分裂为同库下的两张表。例：原1023号库0号表的数据将按照新方案分配到第1023号库0号表和1号表。**迁移方案为：老表数据根据分配算法迁移到同库下的两张表中，应用修改数据库数据的表映射算法。**



**如果第三阶段也达到瓶颈，那么可以按照第二方案再进行扩容。**



#### 结论

这种按照库表比列为`2^n*2^n` 同时尽量保证总表数不变的分配方法对后期扩容和扩容后应用修改是比较友好的。只需简单的数据操作和简单的应用修改就能完成扩容。如果32\*32太大，可以在初期设置为8\*8 或 16\*16 。